function [x522, state] = faceRecFcn(input_1, params, varargin)
%FACERECFCN Function implementing an imported ONNX network.
%
% THIS FILE WAS AUTO-GENERATED BY importONNXFunction.
% ONNX Operator Set Version: 10
%
% Variable names in this function are taken from the original ONNX file.
%
% [X522] = faceRecFcn(INPUT_1, PARAMS)
%			- Evaluates the imported ONNX network FACERECFCN with input(s)
%			INPUT_1 and the imported network parameters in PARAMS. Returns
%			network output(s) in X522.
%
% [X522, STATE] = faceRecFcn(INPUT_1, PARAMS)
%			- Additionally returns state variables in STATE. When training,
%			use this form and set TRAINING to true.
%
% [__] = faceRecFcn(INPUT_1, PARAMS, 'NAME1', VAL1, 'NAME2', VAL2, ...)
%			- Specifies additional name-value pairs described below:
%
% 'Training'
% 			Boolean indicating whether the network is being evaluated for
%			prediction or training. If TRAINING is true, state variables
%			will be updated.
%
% 'InputDataPermutation'
%			'auto' - Automatically attempt to determine the permutation
%			 between the dimensions of the input data and the dimensions of
%			the ONNX model input. For example, the permutation from HWCN
%			(MATLAB standard) to NCHW (ONNX standard) uses the vector
%			[4 3 1 2]. See the documentation for IMPORTONNXFUNCTION for
%			more information about automatic permutation.
%
%			'none' - Input(s) are passed in the ONNX model format. See 'Inputs'.
%
%			numeric vector - The permutation vector describing the
%			transformation between input data dimensions and the expected
%			ONNX input dimensions.%
%			cell array - If the network has multiple inputs, each cell
%			contains 'auto', 'none', or a numeric vector.
%
% 'OutputDataPermutation'
%			'auto' - Automatically attempt to determine the permutation
%			between the dimensions of the output and a conventional MATLAB
%			dimension ordering. For example, the permutation from NC (ONNX
%			standard) to CN (MATLAB standard) uses the vector [2 1]. See
%			the documentation for IMPORTONNXFUNCTION for more information
%			about automatic permutation.
%
%			'none' - Return output(s) as given by the ONNX model. See 'Outputs'.
%
%			numeric vector - The permutation vector describing the
%			transformation between the ONNX output dimensions and the
%			desired output dimensions.%
%			cell array - If the network has multiple outputs, each cell
%			contains 'auto', 'none' or a numeric vector.
%
% Inputs:
% -------
% INPUT_1
%			- Input(s) to the ONNX network.
%			  The input size(s) expected by the ONNX file are:
%				  INPUT_1:		[1, 3, 112, 112]				Type: FLOAT
%			  By default, the function will try to permute the input(s)
%			  into this dimension ordering. If the default is incorrect,
%			  use the 'InputDataPermutation' argument to control the
%			  permutation.
%
%
% PARAMS	- Network parameters returned by 'importONNXFunction'.
%
%
% Outputs:
% --------
% X522
%			- Output(s) of the ONNX network.
%			  Without permutation, the size(s) of the outputs are:
%				  X522:		[1, 512]				Type: FLOAT
%			  By default, the function will try to permute the output(s)
%			  from this dimension ordering into a conventional MATLAB
%			  ordering. If the default is incorrect, use the
%			  'OutputDataPermutation' argument to control the permutation.
%
% STATE		- (Optional) State variables. When TRAINING is true, these will
% 			  have been updated from the original values in PARAMS.State.
%
%
%  See also importONNXFunction

% Preprocess the input data and arguments:
[input_1, Training, outputDataPerms, anyDlarrayInputs] = preprocessInput(input_1, params, varargin{:});
% Put all variables into a single struct to implement dynamic scoping:
[Vars, NumDims] = packageVariables(params, {'input_1'}, {input_1}, [4]);
% Call the top-level graph function:
[x522, NumDims.x522, state] = torch_jit_exportGraph4496(input_1, NumDims.input_1, Vars, NumDims, Training, params.State);
% Postprocess the output data
[x522] = postprocessOutput(x522, outputDataPerms, anyDlarrayInputs, Training, varargin{:});
end

function [x522, x522NumDims4647, state] = torch_jit_exportGraph4496(input_1, input_1NumDims4646, Vars, NumDims, Training, state)
% Function implementing the graph 'torch_jit_exportGraph4496'
% Update Vars and NumDims from the graph's formal input parameters. Note that state variables are already in Vars.
Vars.input_1 = input_1;
NumDims.input_1 = input_1NumDims4646;

% Execute the operators:
% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x334] = prepareConvArgs(Vars.conv1_conv_weight, '', Vars.ConvStride4497, Vars.ConvDilationFactor4498, Vars.ConvPadding4499, 1, NumDims.input_1, NumDims.conv1_conv_weight);
Vars.x334 = dlconv(Vars.input_1, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x335, NumDims.conv1_bn_running_mean, NumDims.conv1_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv1_bn_bias, Vars.conv1_bn_weight, Vars.conv1_bn_running_mean, Vars.conv1_bn_running_var, NumDims.x334, NumDims.conv1_bn_running_mean, NumDims.conv1_bn_running_var);
if Training
    [Vars.x335, dsmean, dsvar] = batchnorm(Vars.x334, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv1_bn_running_mean = dlarray(dsmean);
    Vars.conv1_bn_running_var = dlarray(dsvar);
else
    Vars.x335 = batchnorm(Vars.x334, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv1_bn_running_mean = Vars.conv1_bn_running_mean;
state.conv1_bn_running_var = Vars.conv1_bn_running_var;

% PRelu:
Vars.x337 = max(0,Vars.x335) + Vars.x523.*min(0,Vars.x335);
NumDims.x337 = NumDims.x335;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x338] = prepareConvArgs(Vars.conv2_dw_conv_weight, '', Vars.ConvStride4500, Vars.ConvDilationFactor4501, Vars.ConvPadding4502, 64, NumDims.x337, NumDims.conv2_dw_conv_weight);
Vars.x338 = dlconv(Vars.x337, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x339, NumDims.conv2_dw_bn_running_mean, NumDims.conv2_dw_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv2_dw_bn_bias, Vars.conv2_dw_bn_weight, Vars.conv2_dw_bn_running_mean, Vars.conv2_dw_bn_running_var, NumDims.x338, NumDims.conv2_dw_bn_running_mean, NumDims.conv2_dw_bn_running_var);
if Training
    [Vars.x339, dsmean, dsvar] = batchnorm(Vars.x338, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv2_dw_bn_running_mean = dlarray(dsmean);
    Vars.conv2_dw_bn_running_var = dlarray(dsvar);
else
    Vars.x339 = batchnorm(Vars.x338, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv2_dw_bn_running_mean = Vars.conv2_dw_bn_running_mean;
state.conv2_dw_bn_running_var = Vars.conv2_dw_bn_running_var;

% PRelu:
Vars.x341 = max(0,Vars.x339) + Vars.x524.*min(0,Vars.x339);
NumDims.x341 = NumDims.x339;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x342] = prepareConvArgs(Vars.conv_23_conv_conv_weight, '', Vars.ConvStride4503, Vars.ConvDilationFactor4504, Vars.ConvPadding4505, 1, NumDims.x341, NumDims.conv_23_conv_conv_weight);
Vars.x342 = dlconv(Vars.x341, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x343, NumDims.conv_23_conv_bn_running_mean, NumDims.conv_23_conv_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_23_conv_bn_bias, Vars.conv_23_conv_bn_weight, Vars.conv_23_conv_bn_running_mean, Vars.conv_23_conv_bn_running_var, NumDims.x342, NumDims.conv_23_conv_bn_running_mean, NumDims.conv_23_conv_bn_running_var);
if Training
    [Vars.x343, dsmean, dsvar] = batchnorm(Vars.x342, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_23_conv_bn_running_mean = dlarray(dsmean);
    Vars.conv_23_conv_bn_running_var = dlarray(dsvar);
else
    Vars.x343 = batchnorm(Vars.x342, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_23_conv_bn_running_mean = Vars.conv_23_conv_bn_running_mean;
state.conv_23_conv_bn_running_var = Vars.conv_23_conv_bn_running_var;

% PRelu:
Vars.x345 = max(0,Vars.x343) + Vars.x525.*min(0,Vars.x343);
NumDims.x345 = NumDims.x343;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x346] = prepareConvArgs(Vars.conv_23_conv_dw_conv_weight, '', Vars.ConvStride4506, Vars.ConvDilationFactor4507, Vars.ConvPadding4508, 128, NumDims.x345, NumDims.conv_23_conv_dw_conv_weight);
Vars.x346 = dlconv(Vars.x345, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x347, NumDims.conv_23_conv_dw_bn_running_mean, NumDims.conv_23_conv_dw_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_23_conv_dw_bn_bias, Vars.conv_23_conv_dw_bn_weight, Vars.conv_23_conv_dw_bn_running_mean, Vars.conv_23_conv_dw_bn_running_var, NumDims.x346, NumDims.conv_23_conv_dw_bn_running_mean, NumDims.conv_23_conv_dw_bn_running_var);
if Training
    [Vars.x347, dsmean, dsvar] = batchnorm(Vars.x346, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_23_conv_dw_bn_running_mean = dlarray(dsmean);
    Vars.conv_23_conv_dw_bn_running_var = dlarray(dsvar);
else
    Vars.x347 = batchnorm(Vars.x346, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_23_conv_dw_bn_running_mean = Vars.conv_23_conv_dw_bn_running_mean;
state.conv_23_conv_dw_bn_running_var = Vars.conv_23_conv_dw_bn_running_var;

% PRelu:
Vars.x349 = max(0,Vars.x347) + Vars.x526.*min(0,Vars.x347);
NumDims.x349 = NumDims.x347;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x350] = prepareConvArgs(Vars.conv_23_project_conv_weight, '', Vars.ConvStride4509, Vars.ConvDilationFactor4510, Vars.ConvPadding4511, 1, NumDims.x349, NumDims.conv_23_project_conv_weight);
Vars.x350 = dlconv(Vars.x349, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x351, NumDims.conv_23_project_bn_running_mean, NumDims.conv_23_project_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_23_project_bn_bias, Vars.conv_23_project_bn_weight, Vars.conv_23_project_bn_running_mean, Vars.conv_23_project_bn_running_var, NumDims.x350, NumDims.conv_23_project_bn_running_mean, NumDims.conv_23_project_bn_running_var);
if Training
    [Vars.x351, dsmean, dsvar] = batchnorm(Vars.x350, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_23_project_bn_running_mean = dlarray(dsmean);
    Vars.conv_23_project_bn_running_var = dlarray(dsvar);
else
    Vars.x351 = batchnorm(Vars.x350, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_23_project_bn_running_mean = Vars.conv_23_project_bn_running_mean;
state.conv_23_project_bn_running_var = Vars.conv_23_project_bn_running_var;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x352] = prepareConvArgs(Vars.conv_3_model_0_conv_conv_weight, '', Vars.ConvStride4512, Vars.ConvDilationFactor4513, Vars.ConvPadding4514, 1, NumDims.x351, NumDims.conv_3_model_0_conv_conv_weight);
Vars.x352 = dlconv(Vars.x351, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x353, NumDims.conv_3_model_0_conv_bn_running_mean, NumDims.conv_3_model_0_conv_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_3_model_0_conv_bn_bias, Vars.conv_3_model_0_conv_bn_weight, Vars.conv_3_model_0_conv_bn_running_mean, Vars.conv_3_model_0_conv_bn_running_var, NumDims.x352, NumDims.conv_3_model_0_conv_bn_running_mean, NumDims.conv_3_model_0_conv_bn_running_var);
if Training
    [Vars.x353, dsmean, dsvar] = batchnorm(Vars.x352, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_3_model_0_conv_bn_running_mean = dlarray(dsmean);
    Vars.conv_3_model_0_conv_bn_running_var = dlarray(dsvar);
else
    Vars.x353 = batchnorm(Vars.x352, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_3_model_0_conv_bn_running_mean = Vars.conv_3_model_0_conv_bn_running_mean;
state.conv_3_model_0_conv_bn_running_var = Vars.conv_3_model_0_conv_bn_running_var;

% PRelu:
Vars.x355 = max(0,Vars.x353) + Vars.x527.*min(0,Vars.x353);
NumDims.x355 = NumDims.x353;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x356] = prepareConvArgs(Vars.conv_3_model_0_conv_dw_conv_weight, '', Vars.ConvStride4515, Vars.ConvDilationFactor4516, Vars.ConvPadding4517, 128, NumDims.x355, NumDims.conv_3_model_0_conv_dw_conv_weight);
Vars.x356 = dlconv(Vars.x355, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x357, NumDims.conv_3_model_0_conv_dw_bn_running_mean, NumDims.conv_3_model_0_conv_dw_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_3_model_0_conv_dw_bn_bias, Vars.conv_3_model_0_conv_dw_bn_weight, Vars.conv_3_model_0_conv_dw_bn_running_mean, Vars.conv_3_model_0_conv_dw_bn_running_var, NumDims.x356, NumDims.conv_3_model_0_conv_dw_bn_running_mean, NumDims.conv_3_model_0_conv_dw_bn_running_var);
if Training
    [Vars.x357, dsmean, dsvar] = batchnorm(Vars.x356, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_3_model_0_conv_dw_bn_running_mean = dlarray(dsmean);
    Vars.conv_3_model_0_conv_dw_bn_running_var = dlarray(dsvar);
else
    Vars.x357 = batchnorm(Vars.x356, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_3_model_0_conv_dw_bn_running_mean = Vars.conv_3_model_0_conv_dw_bn_running_mean;
state.conv_3_model_0_conv_dw_bn_running_var = Vars.conv_3_model_0_conv_dw_bn_running_var;

% PRelu:
Vars.x359 = max(0,Vars.x357) + Vars.x528.*min(0,Vars.x357);
NumDims.x359 = NumDims.x357;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x360] = prepareConvArgs(Vars.conv_3_model_0_project_conv_weight, '', Vars.ConvStride4518, Vars.ConvDilationFactor4519, Vars.ConvPadding4520, 1, NumDims.x359, NumDims.conv_3_model_0_project_conv_weight);
Vars.x360 = dlconv(Vars.x359, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x361, NumDims.conv_3_model_0_project_bn_running_mean, NumDims.conv_3_model_0_project_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_3_model_0_project_bn_bias, Vars.conv_3_model_0_project_bn_weight, Vars.conv_3_model_0_project_bn_running_mean, Vars.conv_3_model_0_project_bn_running_var, NumDims.x360, NumDims.conv_3_model_0_project_bn_running_mean, NumDims.conv_3_model_0_project_bn_running_var);
if Training
    [Vars.x361, dsmean, dsvar] = batchnorm(Vars.x360, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_3_model_0_project_bn_running_mean = dlarray(dsmean);
    Vars.conv_3_model_0_project_bn_running_var = dlarray(dsvar);
else
    Vars.x361 = batchnorm(Vars.x360, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_3_model_0_project_bn_running_mean = Vars.conv_3_model_0_project_bn_running_mean;
state.conv_3_model_0_project_bn_running_var = Vars.conv_3_model_0_project_bn_running_var;

% Add:
Vars.x362 = Vars.x351 + Vars.x361;
NumDims.x362 = max(NumDims.x351, NumDims.x361);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x363] = prepareConvArgs(Vars.conv_3_model_1_conv_conv_weight, '', Vars.ConvStride4521, Vars.ConvDilationFactor4522, Vars.ConvPadding4523, 1, NumDims.x362, NumDims.conv_3_model_1_conv_conv_weight);
Vars.x363 = dlconv(Vars.x362, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x364, NumDims.conv_3_model_1_conv_bn_running_mean, NumDims.conv_3_model_1_conv_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_3_model_1_conv_bn_bias, Vars.conv_3_model_1_conv_bn_weight, Vars.conv_3_model_1_conv_bn_running_mean, Vars.conv_3_model_1_conv_bn_running_var, NumDims.x363, NumDims.conv_3_model_1_conv_bn_running_mean, NumDims.conv_3_model_1_conv_bn_running_var);
if Training
    [Vars.x364, dsmean, dsvar] = batchnorm(Vars.x363, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_3_model_1_conv_bn_running_mean = dlarray(dsmean);
    Vars.conv_3_model_1_conv_bn_running_var = dlarray(dsvar);
else
    Vars.x364 = batchnorm(Vars.x363, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_3_model_1_conv_bn_running_mean = Vars.conv_3_model_1_conv_bn_running_mean;
state.conv_3_model_1_conv_bn_running_var = Vars.conv_3_model_1_conv_bn_running_var;

% PRelu:
Vars.x366 = max(0,Vars.x364) + Vars.x529.*min(0,Vars.x364);
NumDims.x366 = NumDims.x364;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x367] = prepareConvArgs(Vars.conv_3_model_1_conv_dw_conv_weight, '', Vars.ConvStride4524, Vars.ConvDilationFactor4525, Vars.ConvPadding4526, 128, NumDims.x366, NumDims.conv_3_model_1_conv_dw_conv_weight);
Vars.x367 = dlconv(Vars.x366, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x368, NumDims.conv_3_model_1_conv_dw_bn_running_mean, NumDims.conv_3_model_1_conv_dw_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_3_model_1_conv_dw_bn_bias, Vars.conv_3_model_1_conv_dw_bn_weight, Vars.conv_3_model_1_conv_dw_bn_running_mean, Vars.conv_3_model_1_conv_dw_bn_running_var, NumDims.x367, NumDims.conv_3_model_1_conv_dw_bn_running_mean, NumDims.conv_3_model_1_conv_dw_bn_running_var);
if Training
    [Vars.x368, dsmean, dsvar] = batchnorm(Vars.x367, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_3_model_1_conv_dw_bn_running_mean = dlarray(dsmean);
    Vars.conv_3_model_1_conv_dw_bn_running_var = dlarray(dsvar);
else
    Vars.x368 = batchnorm(Vars.x367, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_3_model_1_conv_dw_bn_running_mean = Vars.conv_3_model_1_conv_dw_bn_running_mean;
state.conv_3_model_1_conv_dw_bn_running_var = Vars.conv_3_model_1_conv_dw_bn_running_var;

% PRelu:
Vars.x370 = max(0,Vars.x368) + Vars.x530.*min(0,Vars.x368);
NumDims.x370 = NumDims.x368;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x371] = prepareConvArgs(Vars.conv_3_model_1_project_conv_weight, '', Vars.ConvStride4527, Vars.ConvDilationFactor4528, Vars.ConvPadding4529, 1, NumDims.x370, NumDims.conv_3_model_1_project_conv_weight);
Vars.x371 = dlconv(Vars.x370, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x372, NumDims.conv_3_model_1_project_bn_running_mean, NumDims.conv_3_model_1_project_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_3_model_1_project_bn_bias, Vars.conv_3_model_1_project_bn_weight, Vars.conv_3_model_1_project_bn_running_mean, Vars.conv_3_model_1_project_bn_running_var, NumDims.x371, NumDims.conv_3_model_1_project_bn_running_mean, NumDims.conv_3_model_1_project_bn_running_var);
if Training
    [Vars.x372, dsmean, dsvar] = batchnorm(Vars.x371, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_3_model_1_project_bn_running_mean = dlarray(dsmean);
    Vars.conv_3_model_1_project_bn_running_var = dlarray(dsvar);
else
    Vars.x372 = batchnorm(Vars.x371, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_3_model_1_project_bn_running_mean = Vars.conv_3_model_1_project_bn_running_mean;
state.conv_3_model_1_project_bn_running_var = Vars.conv_3_model_1_project_bn_running_var;

% Add:
Vars.x373 = Vars.x362 + Vars.x372;
NumDims.x373 = max(NumDims.x362, NumDims.x372);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x374] = prepareConvArgs(Vars.conv_3_model_2_conv_conv_weight, '', Vars.ConvStride4530, Vars.ConvDilationFactor4531, Vars.ConvPadding4532, 1, NumDims.x373, NumDims.conv_3_model_2_conv_conv_weight);
Vars.x374 = dlconv(Vars.x373, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x375, NumDims.conv_3_model_2_conv_bn_running_mean, NumDims.conv_3_model_2_conv_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_3_model_2_conv_bn_bias, Vars.conv_3_model_2_conv_bn_weight, Vars.conv_3_model_2_conv_bn_running_mean, Vars.conv_3_model_2_conv_bn_running_var, NumDims.x374, NumDims.conv_3_model_2_conv_bn_running_mean, NumDims.conv_3_model_2_conv_bn_running_var);
if Training
    [Vars.x375, dsmean, dsvar] = batchnorm(Vars.x374, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_3_model_2_conv_bn_running_mean = dlarray(dsmean);
    Vars.conv_3_model_2_conv_bn_running_var = dlarray(dsvar);
else
    Vars.x375 = batchnorm(Vars.x374, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_3_model_2_conv_bn_running_mean = Vars.conv_3_model_2_conv_bn_running_mean;
state.conv_3_model_2_conv_bn_running_var = Vars.conv_3_model_2_conv_bn_running_var;

% PRelu:
Vars.x377 = max(0,Vars.x375) + Vars.x531.*min(0,Vars.x375);
NumDims.x377 = NumDims.x375;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x378] = prepareConvArgs(Vars.conv_3_model_2_conv_dw_conv_weight, '', Vars.ConvStride4533, Vars.ConvDilationFactor4534, Vars.ConvPadding4535, 128, NumDims.x377, NumDims.conv_3_model_2_conv_dw_conv_weight);
Vars.x378 = dlconv(Vars.x377, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x379, NumDims.conv_3_model_2_conv_dw_bn_running_mean, NumDims.conv_3_model_2_conv_dw_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_3_model_2_conv_dw_bn_bias, Vars.conv_3_model_2_conv_dw_bn_weight, Vars.conv_3_model_2_conv_dw_bn_running_mean, Vars.conv_3_model_2_conv_dw_bn_running_var, NumDims.x378, NumDims.conv_3_model_2_conv_dw_bn_running_mean, NumDims.conv_3_model_2_conv_dw_bn_running_var);
if Training
    [Vars.x379, dsmean, dsvar] = batchnorm(Vars.x378, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_3_model_2_conv_dw_bn_running_mean = dlarray(dsmean);
    Vars.conv_3_model_2_conv_dw_bn_running_var = dlarray(dsvar);
else
    Vars.x379 = batchnorm(Vars.x378, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_3_model_2_conv_dw_bn_running_mean = Vars.conv_3_model_2_conv_dw_bn_running_mean;
state.conv_3_model_2_conv_dw_bn_running_var = Vars.conv_3_model_2_conv_dw_bn_running_var;

% PRelu:
Vars.x381 = max(0,Vars.x379) + Vars.x532.*min(0,Vars.x379);
NumDims.x381 = NumDims.x379;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x382] = prepareConvArgs(Vars.conv_3_model_2_project_conv_weight, '', Vars.ConvStride4536, Vars.ConvDilationFactor4537, Vars.ConvPadding4538, 1, NumDims.x381, NumDims.conv_3_model_2_project_conv_weight);
Vars.x382 = dlconv(Vars.x381, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x383, NumDims.conv_3_model_2_project_bn_running_mean, NumDims.conv_3_model_2_project_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_3_model_2_project_bn_bias, Vars.conv_3_model_2_project_bn_weight, Vars.conv_3_model_2_project_bn_running_mean, Vars.conv_3_model_2_project_bn_running_var, NumDims.x382, NumDims.conv_3_model_2_project_bn_running_mean, NumDims.conv_3_model_2_project_bn_running_var);
if Training
    [Vars.x383, dsmean, dsvar] = batchnorm(Vars.x382, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_3_model_2_project_bn_running_mean = dlarray(dsmean);
    Vars.conv_3_model_2_project_bn_running_var = dlarray(dsvar);
else
    Vars.x383 = batchnorm(Vars.x382, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_3_model_2_project_bn_running_mean = Vars.conv_3_model_2_project_bn_running_mean;
state.conv_3_model_2_project_bn_running_var = Vars.conv_3_model_2_project_bn_running_var;

% Add:
Vars.x384 = Vars.x373 + Vars.x383;
NumDims.x384 = max(NumDims.x373, NumDims.x383);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x385] = prepareConvArgs(Vars.conv_3_model_3_conv_conv_weight, '', Vars.ConvStride4539, Vars.ConvDilationFactor4540, Vars.ConvPadding4541, 1, NumDims.x384, NumDims.conv_3_model_3_conv_conv_weight);
Vars.x385 = dlconv(Vars.x384, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x386, NumDims.conv_3_model_3_conv_bn_running_mean, NumDims.conv_3_model_3_conv_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_3_model_3_conv_bn_bias, Vars.conv_3_model_3_conv_bn_weight, Vars.conv_3_model_3_conv_bn_running_mean, Vars.conv_3_model_3_conv_bn_running_var, NumDims.x385, NumDims.conv_3_model_3_conv_bn_running_mean, NumDims.conv_3_model_3_conv_bn_running_var);
if Training
    [Vars.x386, dsmean, dsvar] = batchnorm(Vars.x385, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_3_model_3_conv_bn_running_mean = dlarray(dsmean);
    Vars.conv_3_model_3_conv_bn_running_var = dlarray(dsvar);
else
    Vars.x386 = batchnorm(Vars.x385, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_3_model_3_conv_bn_running_mean = Vars.conv_3_model_3_conv_bn_running_mean;
state.conv_3_model_3_conv_bn_running_var = Vars.conv_3_model_3_conv_bn_running_var;

% PRelu:
Vars.x388 = max(0,Vars.x386) + Vars.x533.*min(0,Vars.x386);
NumDims.x388 = NumDims.x386;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x389] = prepareConvArgs(Vars.conv_3_model_3_conv_dw_conv_weight, '', Vars.ConvStride4542, Vars.ConvDilationFactor4543, Vars.ConvPadding4544, 128, NumDims.x388, NumDims.conv_3_model_3_conv_dw_conv_weight);
Vars.x389 = dlconv(Vars.x388, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x390, NumDims.conv_3_model_3_conv_dw_bn_running_mean, NumDims.conv_3_model_3_conv_dw_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_3_model_3_conv_dw_bn_bias, Vars.conv_3_model_3_conv_dw_bn_weight, Vars.conv_3_model_3_conv_dw_bn_running_mean, Vars.conv_3_model_3_conv_dw_bn_running_var, NumDims.x389, NumDims.conv_3_model_3_conv_dw_bn_running_mean, NumDims.conv_3_model_3_conv_dw_bn_running_var);
if Training
    [Vars.x390, dsmean, dsvar] = batchnorm(Vars.x389, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_3_model_3_conv_dw_bn_running_mean = dlarray(dsmean);
    Vars.conv_3_model_3_conv_dw_bn_running_var = dlarray(dsvar);
else
    Vars.x390 = batchnorm(Vars.x389, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_3_model_3_conv_dw_bn_running_mean = Vars.conv_3_model_3_conv_dw_bn_running_mean;
state.conv_3_model_3_conv_dw_bn_running_var = Vars.conv_3_model_3_conv_dw_bn_running_var;

% PRelu:
Vars.x392 = max(0,Vars.x390) + Vars.x534.*min(0,Vars.x390);
NumDims.x392 = NumDims.x390;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x393] = prepareConvArgs(Vars.conv_3_model_3_project_conv_weight, '', Vars.ConvStride4545, Vars.ConvDilationFactor4546, Vars.ConvPadding4547, 1, NumDims.x392, NumDims.conv_3_model_3_project_conv_weight);
Vars.x393 = dlconv(Vars.x392, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x394, NumDims.conv_3_model_3_project_bn_running_mean, NumDims.conv_3_model_3_project_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_3_model_3_project_bn_bias, Vars.conv_3_model_3_project_bn_weight, Vars.conv_3_model_3_project_bn_running_mean, Vars.conv_3_model_3_project_bn_running_var, NumDims.x393, NumDims.conv_3_model_3_project_bn_running_mean, NumDims.conv_3_model_3_project_bn_running_var);
if Training
    [Vars.x394, dsmean, dsvar] = batchnorm(Vars.x393, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_3_model_3_project_bn_running_mean = dlarray(dsmean);
    Vars.conv_3_model_3_project_bn_running_var = dlarray(dsvar);
else
    Vars.x394 = batchnorm(Vars.x393, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_3_model_3_project_bn_running_mean = Vars.conv_3_model_3_project_bn_running_mean;
state.conv_3_model_3_project_bn_running_var = Vars.conv_3_model_3_project_bn_running_var;

% Add:
Vars.x395 = Vars.x384 + Vars.x394;
NumDims.x395 = max(NumDims.x384, NumDims.x394);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x396] = prepareConvArgs(Vars.conv_34_conv_conv_weight, '', Vars.ConvStride4548, Vars.ConvDilationFactor4549, Vars.ConvPadding4550, 1, NumDims.x395, NumDims.conv_34_conv_conv_weight);
Vars.x396 = dlconv(Vars.x395, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x397, NumDims.conv_34_conv_bn_running_mean, NumDims.conv_34_conv_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_34_conv_bn_bias, Vars.conv_34_conv_bn_weight, Vars.conv_34_conv_bn_running_mean, Vars.conv_34_conv_bn_running_var, NumDims.x396, NumDims.conv_34_conv_bn_running_mean, NumDims.conv_34_conv_bn_running_var);
if Training
    [Vars.x397, dsmean, dsvar] = batchnorm(Vars.x396, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_34_conv_bn_running_mean = dlarray(dsmean);
    Vars.conv_34_conv_bn_running_var = dlarray(dsvar);
else
    Vars.x397 = batchnorm(Vars.x396, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_34_conv_bn_running_mean = Vars.conv_34_conv_bn_running_mean;
state.conv_34_conv_bn_running_var = Vars.conv_34_conv_bn_running_var;

% PRelu:
Vars.x399 = max(0,Vars.x397) + Vars.x535.*min(0,Vars.x397);
NumDims.x399 = NumDims.x397;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x400] = prepareConvArgs(Vars.conv_34_conv_dw_conv_weight, '', Vars.ConvStride4551, Vars.ConvDilationFactor4552, Vars.ConvPadding4553, 256, NumDims.x399, NumDims.conv_34_conv_dw_conv_weight);
Vars.x400 = dlconv(Vars.x399, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x401, NumDims.conv_34_conv_dw_bn_running_mean, NumDims.conv_34_conv_dw_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_34_conv_dw_bn_bias, Vars.conv_34_conv_dw_bn_weight, Vars.conv_34_conv_dw_bn_running_mean, Vars.conv_34_conv_dw_bn_running_var, NumDims.x400, NumDims.conv_34_conv_dw_bn_running_mean, NumDims.conv_34_conv_dw_bn_running_var);
if Training
    [Vars.x401, dsmean, dsvar] = batchnorm(Vars.x400, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_34_conv_dw_bn_running_mean = dlarray(dsmean);
    Vars.conv_34_conv_dw_bn_running_var = dlarray(dsvar);
else
    Vars.x401 = batchnorm(Vars.x400, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_34_conv_dw_bn_running_mean = Vars.conv_34_conv_dw_bn_running_mean;
state.conv_34_conv_dw_bn_running_var = Vars.conv_34_conv_dw_bn_running_var;

% PRelu:
Vars.x403 = max(0,Vars.x401) + Vars.x536.*min(0,Vars.x401);
NumDims.x403 = NumDims.x401;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x404] = prepareConvArgs(Vars.conv_34_project_conv_weight, '', Vars.ConvStride4554, Vars.ConvDilationFactor4555, Vars.ConvPadding4556, 1, NumDims.x403, NumDims.conv_34_project_conv_weight);
Vars.x404 = dlconv(Vars.x403, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x405, NumDims.conv_34_project_bn_running_mean, NumDims.conv_34_project_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_34_project_bn_bias, Vars.conv_34_project_bn_weight, Vars.conv_34_project_bn_running_mean, Vars.conv_34_project_bn_running_var, NumDims.x404, NumDims.conv_34_project_bn_running_mean, NumDims.conv_34_project_bn_running_var);
if Training
    [Vars.x405, dsmean, dsvar] = batchnorm(Vars.x404, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_34_project_bn_running_mean = dlarray(dsmean);
    Vars.conv_34_project_bn_running_var = dlarray(dsvar);
else
    Vars.x405 = batchnorm(Vars.x404, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_34_project_bn_running_mean = Vars.conv_34_project_bn_running_mean;
state.conv_34_project_bn_running_var = Vars.conv_34_project_bn_running_var;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x406] = prepareConvArgs(Vars.conv_4_model_0_conv_conv_weight, '', Vars.ConvStride4557, Vars.ConvDilationFactor4558, Vars.ConvPadding4559, 1, NumDims.x405, NumDims.conv_4_model_0_conv_conv_weight);
Vars.x406 = dlconv(Vars.x405, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x407, NumDims.conv_4_model_0_conv_bn_running_mean, NumDims.conv_4_model_0_conv_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_4_model_0_conv_bn_bias, Vars.conv_4_model_0_conv_bn_weight, Vars.conv_4_model_0_conv_bn_running_mean, Vars.conv_4_model_0_conv_bn_running_var, NumDims.x406, NumDims.conv_4_model_0_conv_bn_running_mean, NumDims.conv_4_model_0_conv_bn_running_var);
if Training
    [Vars.x407, dsmean, dsvar] = batchnorm(Vars.x406, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_4_model_0_conv_bn_running_mean = dlarray(dsmean);
    Vars.conv_4_model_0_conv_bn_running_var = dlarray(dsvar);
else
    Vars.x407 = batchnorm(Vars.x406, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_4_model_0_conv_bn_running_mean = Vars.conv_4_model_0_conv_bn_running_mean;
state.conv_4_model_0_conv_bn_running_var = Vars.conv_4_model_0_conv_bn_running_var;

% PRelu:
Vars.x409 = max(0,Vars.x407) + Vars.x537.*min(0,Vars.x407);
NumDims.x409 = NumDims.x407;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x410] = prepareConvArgs(Vars.conv_4_model_0_conv_dw_conv_weight, '', Vars.ConvStride4560, Vars.ConvDilationFactor4561, Vars.ConvPadding4562, 256, NumDims.x409, NumDims.conv_4_model_0_conv_dw_conv_weight);
Vars.x410 = dlconv(Vars.x409, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x411, NumDims.conv_4_model_0_conv_dw_bn_running_mean, NumDims.conv_4_model_0_conv_dw_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_4_model_0_conv_dw_bn_bias, Vars.conv_4_model_0_conv_dw_bn_weight, Vars.conv_4_model_0_conv_dw_bn_running_mean, Vars.conv_4_model_0_conv_dw_bn_running_var, NumDims.x410, NumDims.conv_4_model_0_conv_dw_bn_running_mean, NumDims.conv_4_model_0_conv_dw_bn_running_var);
if Training
    [Vars.x411, dsmean, dsvar] = batchnorm(Vars.x410, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_4_model_0_conv_dw_bn_running_mean = dlarray(dsmean);
    Vars.conv_4_model_0_conv_dw_bn_running_var = dlarray(dsvar);
else
    Vars.x411 = batchnorm(Vars.x410, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_4_model_0_conv_dw_bn_running_mean = Vars.conv_4_model_0_conv_dw_bn_running_mean;
state.conv_4_model_0_conv_dw_bn_running_var = Vars.conv_4_model_0_conv_dw_bn_running_var;

% PRelu:
Vars.x413 = max(0,Vars.x411) + Vars.x538.*min(0,Vars.x411);
NumDims.x413 = NumDims.x411;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x414] = prepareConvArgs(Vars.conv_4_model_0_project_conv_weight, '', Vars.ConvStride4563, Vars.ConvDilationFactor4564, Vars.ConvPadding4565, 1, NumDims.x413, NumDims.conv_4_model_0_project_conv_weight);
Vars.x414 = dlconv(Vars.x413, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x415, NumDims.conv_4_model_0_project_bn_running_mean, NumDims.conv_4_model_0_project_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_4_model_0_project_bn_bias, Vars.conv_4_model_0_project_bn_weight, Vars.conv_4_model_0_project_bn_running_mean, Vars.conv_4_model_0_project_bn_running_var, NumDims.x414, NumDims.conv_4_model_0_project_bn_running_mean, NumDims.conv_4_model_0_project_bn_running_var);
if Training
    [Vars.x415, dsmean, dsvar] = batchnorm(Vars.x414, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_4_model_0_project_bn_running_mean = dlarray(dsmean);
    Vars.conv_4_model_0_project_bn_running_var = dlarray(dsvar);
else
    Vars.x415 = batchnorm(Vars.x414, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_4_model_0_project_bn_running_mean = Vars.conv_4_model_0_project_bn_running_mean;
state.conv_4_model_0_project_bn_running_var = Vars.conv_4_model_0_project_bn_running_var;

% Add:
Vars.x416 = Vars.x405 + Vars.x415;
NumDims.x416 = max(NumDims.x405, NumDims.x415);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x417] = prepareConvArgs(Vars.conv_4_model_1_conv_conv_weight, '', Vars.ConvStride4566, Vars.ConvDilationFactor4567, Vars.ConvPadding4568, 1, NumDims.x416, NumDims.conv_4_model_1_conv_conv_weight);
Vars.x417 = dlconv(Vars.x416, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x418, NumDims.conv_4_model_1_conv_bn_running_mean, NumDims.conv_4_model_1_conv_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_4_model_1_conv_bn_bias, Vars.conv_4_model_1_conv_bn_weight, Vars.conv_4_model_1_conv_bn_running_mean, Vars.conv_4_model_1_conv_bn_running_var, NumDims.x417, NumDims.conv_4_model_1_conv_bn_running_mean, NumDims.conv_4_model_1_conv_bn_running_var);
if Training
    [Vars.x418, dsmean, dsvar] = batchnorm(Vars.x417, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_4_model_1_conv_bn_running_mean = dlarray(dsmean);
    Vars.conv_4_model_1_conv_bn_running_var = dlarray(dsvar);
else
    Vars.x418 = batchnorm(Vars.x417, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_4_model_1_conv_bn_running_mean = Vars.conv_4_model_1_conv_bn_running_mean;
state.conv_4_model_1_conv_bn_running_var = Vars.conv_4_model_1_conv_bn_running_var;

% PRelu:
Vars.x420 = max(0,Vars.x418) + Vars.x539.*min(0,Vars.x418);
NumDims.x420 = NumDims.x418;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x421] = prepareConvArgs(Vars.conv_4_model_1_conv_dw_conv_weight, '', Vars.ConvStride4569, Vars.ConvDilationFactor4570, Vars.ConvPadding4571, 256, NumDims.x420, NumDims.conv_4_model_1_conv_dw_conv_weight);
Vars.x421 = dlconv(Vars.x420, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x422, NumDims.conv_4_model_1_conv_dw_bn_running_mean, NumDims.conv_4_model_1_conv_dw_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_4_model_1_conv_dw_bn_bias, Vars.conv_4_model_1_conv_dw_bn_weight, Vars.conv_4_model_1_conv_dw_bn_running_mean, Vars.conv_4_model_1_conv_dw_bn_running_var, NumDims.x421, NumDims.conv_4_model_1_conv_dw_bn_running_mean, NumDims.conv_4_model_1_conv_dw_bn_running_var);
if Training
    [Vars.x422, dsmean, dsvar] = batchnorm(Vars.x421, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_4_model_1_conv_dw_bn_running_mean = dlarray(dsmean);
    Vars.conv_4_model_1_conv_dw_bn_running_var = dlarray(dsvar);
else
    Vars.x422 = batchnorm(Vars.x421, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_4_model_1_conv_dw_bn_running_mean = Vars.conv_4_model_1_conv_dw_bn_running_mean;
state.conv_4_model_1_conv_dw_bn_running_var = Vars.conv_4_model_1_conv_dw_bn_running_var;

% PRelu:
Vars.x424 = max(0,Vars.x422) + Vars.x540.*min(0,Vars.x422);
NumDims.x424 = NumDims.x422;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x425] = prepareConvArgs(Vars.conv_4_model_1_project_conv_weight, '', Vars.ConvStride4572, Vars.ConvDilationFactor4573, Vars.ConvPadding4574, 1, NumDims.x424, NumDims.conv_4_model_1_project_conv_weight);
Vars.x425 = dlconv(Vars.x424, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x426, NumDims.conv_4_model_1_project_bn_running_mean, NumDims.conv_4_model_1_project_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_4_model_1_project_bn_bias, Vars.conv_4_model_1_project_bn_weight, Vars.conv_4_model_1_project_bn_running_mean, Vars.conv_4_model_1_project_bn_running_var, NumDims.x425, NumDims.conv_4_model_1_project_bn_running_mean, NumDims.conv_4_model_1_project_bn_running_var);
if Training
    [Vars.x426, dsmean, dsvar] = batchnorm(Vars.x425, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_4_model_1_project_bn_running_mean = dlarray(dsmean);
    Vars.conv_4_model_1_project_bn_running_var = dlarray(dsvar);
else
    Vars.x426 = batchnorm(Vars.x425, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_4_model_1_project_bn_running_mean = Vars.conv_4_model_1_project_bn_running_mean;
state.conv_4_model_1_project_bn_running_var = Vars.conv_4_model_1_project_bn_running_var;

% Add:
Vars.x427 = Vars.x416 + Vars.x426;
NumDims.x427 = max(NumDims.x416, NumDims.x426);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x428] = prepareConvArgs(Vars.conv_4_model_2_conv_conv_weight, '', Vars.ConvStride4575, Vars.ConvDilationFactor4576, Vars.ConvPadding4577, 1, NumDims.x427, NumDims.conv_4_model_2_conv_conv_weight);
Vars.x428 = dlconv(Vars.x427, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x429, NumDims.conv_4_model_2_conv_bn_running_mean, NumDims.conv_4_model_2_conv_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_4_model_2_conv_bn_bias, Vars.conv_4_model_2_conv_bn_weight, Vars.conv_4_model_2_conv_bn_running_mean, Vars.conv_4_model_2_conv_bn_running_var, NumDims.x428, NumDims.conv_4_model_2_conv_bn_running_mean, NumDims.conv_4_model_2_conv_bn_running_var);
if Training
    [Vars.x429, dsmean, dsvar] = batchnorm(Vars.x428, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_4_model_2_conv_bn_running_mean = dlarray(dsmean);
    Vars.conv_4_model_2_conv_bn_running_var = dlarray(dsvar);
else
    Vars.x429 = batchnorm(Vars.x428, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_4_model_2_conv_bn_running_mean = Vars.conv_4_model_2_conv_bn_running_mean;
state.conv_4_model_2_conv_bn_running_var = Vars.conv_4_model_2_conv_bn_running_var;

% PRelu:
Vars.x431 = max(0,Vars.x429) + Vars.x541.*min(0,Vars.x429);
NumDims.x431 = NumDims.x429;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x432] = prepareConvArgs(Vars.conv_4_model_2_conv_dw_conv_weight, '', Vars.ConvStride4578, Vars.ConvDilationFactor4579, Vars.ConvPadding4580, 256, NumDims.x431, NumDims.conv_4_model_2_conv_dw_conv_weight);
Vars.x432 = dlconv(Vars.x431, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x433, NumDims.conv_4_model_2_conv_dw_bn_running_mean, NumDims.conv_4_model_2_conv_dw_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_4_model_2_conv_dw_bn_bias, Vars.conv_4_model_2_conv_dw_bn_weight, Vars.conv_4_model_2_conv_dw_bn_running_mean, Vars.conv_4_model_2_conv_dw_bn_running_var, NumDims.x432, NumDims.conv_4_model_2_conv_dw_bn_running_mean, NumDims.conv_4_model_2_conv_dw_bn_running_var);
if Training
    [Vars.x433, dsmean, dsvar] = batchnorm(Vars.x432, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_4_model_2_conv_dw_bn_running_mean = dlarray(dsmean);
    Vars.conv_4_model_2_conv_dw_bn_running_var = dlarray(dsvar);
else
    Vars.x433 = batchnorm(Vars.x432, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_4_model_2_conv_dw_bn_running_mean = Vars.conv_4_model_2_conv_dw_bn_running_mean;
state.conv_4_model_2_conv_dw_bn_running_var = Vars.conv_4_model_2_conv_dw_bn_running_var;

% PRelu:
Vars.x435 = max(0,Vars.x433) + Vars.x542.*min(0,Vars.x433);
NumDims.x435 = NumDims.x433;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x436] = prepareConvArgs(Vars.conv_4_model_2_project_conv_weight, '', Vars.ConvStride4581, Vars.ConvDilationFactor4582, Vars.ConvPadding4583, 1, NumDims.x435, NumDims.conv_4_model_2_project_conv_weight);
Vars.x436 = dlconv(Vars.x435, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x437, NumDims.conv_4_model_2_project_bn_running_mean, NumDims.conv_4_model_2_project_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_4_model_2_project_bn_bias, Vars.conv_4_model_2_project_bn_weight, Vars.conv_4_model_2_project_bn_running_mean, Vars.conv_4_model_2_project_bn_running_var, NumDims.x436, NumDims.conv_4_model_2_project_bn_running_mean, NumDims.conv_4_model_2_project_bn_running_var);
if Training
    [Vars.x437, dsmean, dsvar] = batchnorm(Vars.x436, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_4_model_2_project_bn_running_mean = dlarray(dsmean);
    Vars.conv_4_model_2_project_bn_running_var = dlarray(dsvar);
else
    Vars.x437 = batchnorm(Vars.x436, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_4_model_2_project_bn_running_mean = Vars.conv_4_model_2_project_bn_running_mean;
state.conv_4_model_2_project_bn_running_var = Vars.conv_4_model_2_project_bn_running_var;

% Add:
Vars.x438 = Vars.x427 + Vars.x437;
NumDims.x438 = max(NumDims.x427, NumDims.x437);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x439] = prepareConvArgs(Vars.conv_4_model_3_conv_conv_weight, '', Vars.ConvStride4584, Vars.ConvDilationFactor4585, Vars.ConvPadding4586, 1, NumDims.x438, NumDims.conv_4_model_3_conv_conv_weight);
Vars.x439 = dlconv(Vars.x438, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x440, NumDims.conv_4_model_3_conv_bn_running_mean, NumDims.conv_4_model_3_conv_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_4_model_3_conv_bn_bias, Vars.conv_4_model_3_conv_bn_weight, Vars.conv_4_model_3_conv_bn_running_mean, Vars.conv_4_model_3_conv_bn_running_var, NumDims.x439, NumDims.conv_4_model_3_conv_bn_running_mean, NumDims.conv_4_model_3_conv_bn_running_var);
if Training
    [Vars.x440, dsmean, dsvar] = batchnorm(Vars.x439, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_4_model_3_conv_bn_running_mean = dlarray(dsmean);
    Vars.conv_4_model_3_conv_bn_running_var = dlarray(dsvar);
else
    Vars.x440 = batchnorm(Vars.x439, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_4_model_3_conv_bn_running_mean = Vars.conv_4_model_3_conv_bn_running_mean;
state.conv_4_model_3_conv_bn_running_var = Vars.conv_4_model_3_conv_bn_running_var;

% PRelu:
Vars.x442 = max(0,Vars.x440) + Vars.x543.*min(0,Vars.x440);
NumDims.x442 = NumDims.x440;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x443] = prepareConvArgs(Vars.conv_4_model_3_conv_dw_conv_weight, '', Vars.ConvStride4587, Vars.ConvDilationFactor4588, Vars.ConvPadding4589, 256, NumDims.x442, NumDims.conv_4_model_3_conv_dw_conv_weight);
Vars.x443 = dlconv(Vars.x442, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x444, NumDims.conv_4_model_3_conv_dw_bn_running_mean, NumDims.conv_4_model_3_conv_dw_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_4_model_3_conv_dw_bn_bias, Vars.conv_4_model_3_conv_dw_bn_weight, Vars.conv_4_model_3_conv_dw_bn_running_mean, Vars.conv_4_model_3_conv_dw_bn_running_var, NumDims.x443, NumDims.conv_4_model_3_conv_dw_bn_running_mean, NumDims.conv_4_model_3_conv_dw_bn_running_var);
if Training
    [Vars.x444, dsmean, dsvar] = batchnorm(Vars.x443, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_4_model_3_conv_dw_bn_running_mean = dlarray(dsmean);
    Vars.conv_4_model_3_conv_dw_bn_running_var = dlarray(dsvar);
else
    Vars.x444 = batchnorm(Vars.x443, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_4_model_3_conv_dw_bn_running_mean = Vars.conv_4_model_3_conv_dw_bn_running_mean;
state.conv_4_model_3_conv_dw_bn_running_var = Vars.conv_4_model_3_conv_dw_bn_running_var;

% PRelu:
Vars.x446 = max(0,Vars.x444) + Vars.x544.*min(0,Vars.x444);
NumDims.x446 = NumDims.x444;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x447] = prepareConvArgs(Vars.conv_4_model_3_project_conv_weight, '', Vars.ConvStride4590, Vars.ConvDilationFactor4591, Vars.ConvPadding4592, 1, NumDims.x446, NumDims.conv_4_model_3_project_conv_weight);
Vars.x447 = dlconv(Vars.x446, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x448, NumDims.conv_4_model_3_project_bn_running_mean, NumDims.conv_4_model_3_project_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_4_model_3_project_bn_bias, Vars.conv_4_model_3_project_bn_weight, Vars.conv_4_model_3_project_bn_running_mean, Vars.conv_4_model_3_project_bn_running_var, NumDims.x447, NumDims.conv_4_model_3_project_bn_running_mean, NumDims.conv_4_model_3_project_bn_running_var);
if Training
    [Vars.x448, dsmean, dsvar] = batchnorm(Vars.x447, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_4_model_3_project_bn_running_mean = dlarray(dsmean);
    Vars.conv_4_model_3_project_bn_running_var = dlarray(dsvar);
else
    Vars.x448 = batchnorm(Vars.x447, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_4_model_3_project_bn_running_mean = Vars.conv_4_model_3_project_bn_running_mean;
state.conv_4_model_3_project_bn_running_var = Vars.conv_4_model_3_project_bn_running_var;

% Add:
Vars.x449 = Vars.x438 + Vars.x448;
NumDims.x449 = max(NumDims.x438, NumDims.x448);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x450] = prepareConvArgs(Vars.conv_4_model_4_conv_conv_weight, '', Vars.ConvStride4593, Vars.ConvDilationFactor4594, Vars.ConvPadding4595, 1, NumDims.x449, NumDims.conv_4_model_4_conv_conv_weight);
Vars.x450 = dlconv(Vars.x449, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x451, NumDims.conv_4_model_4_conv_bn_running_mean, NumDims.conv_4_model_4_conv_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_4_model_4_conv_bn_bias, Vars.conv_4_model_4_conv_bn_weight, Vars.conv_4_model_4_conv_bn_running_mean, Vars.conv_4_model_4_conv_bn_running_var, NumDims.x450, NumDims.conv_4_model_4_conv_bn_running_mean, NumDims.conv_4_model_4_conv_bn_running_var);
if Training
    [Vars.x451, dsmean, dsvar] = batchnorm(Vars.x450, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_4_model_4_conv_bn_running_mean = dlarray(dsmean);
    Vars.conv_4_model_4_conv_bn_running_var = dlarray(dsvar);
else
    Vars.x451 = batchnorm(Vars.x450, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_4_model_4_conv_bn_running_mean = Vars.conv_4_model_4_conv_bn_running_mean;
state.conv_4_model_4_conv_bn_running_var = Vars.conv_4_model_4_conv_bn_running_var;

% PRelu:
Vars.x453 = max(0,Vars.x451) + Vars.x545.*min(0,Vars.x451);
NumDims.x453 = NumDims.x451;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x454] = prepareConvArgs(Vars.conv_4_model_4_conv_dw_conv_weight, '', Vars.ConvStride4596, Vars.ConvDilationFactor4597, Vars.ConvPadding4598, 256, NumDims.x453, NumDims.conv_4_model_4_conv_dw_conv_weight);
Vars.x454 = dlconv(Vars.x453, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x455, NumDims.conv_4_model_4_conv_dw_bn_running_mean, NumDims.conv_4_model_4_conv_dw_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_4_model_4_conv_dw_bn_bias, Vars.conv_4_model_4_conv_dw_bn_weight, Vars.conv_4_model_4_conv_dw_bn_running_mean, Vars.conv_4_model_4_conv_dw_bn_running_var, NumDims.x454, NumDims.conv_4_model_4_conv_dw_bn_running_mean, NumDims.conv_4_model_4_conv_dw_bn_running_var);
if Training
    [Vars.x455, dsmean, dsvar] = batchnorm(Vars.x454, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_4_model_4_conv_dw_bn_running_mean = dlarray(dsmean);
    Vars.conv_4_model_4_conv_dw_bn_running_var = dlarray(dsvar);
else
    Vars.x455 = batchnorm(Vars.x454, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_4_model_4_conv_dw_bn_running_mean = Vars.conv_4_model_4_conv_dw_bn_running_mean;
state.conv_4_model_4_conv_dw_bn_running_var = Vars.conv_4_model_4_conv_dw_bn_running_var;

% PRelu:
Vars.x457 = max(0,Vars.x455) + Vars.x546.*min(0,Vars.x455);
NumDims.x457 = NumDims.x455;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x458] = prepareConvArgs(Vars.conv_4_model_4_project_conv_weight, '', Vars.ConvStride4599, Vars.ConvDilationFactor4600, Vars.ConvPadding4601, 1, NumDims.x457, NumDims.conv_4_model_4_project_conv_weight);
Vars.x458 = dlconv(Vars.x457, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x459, NumDims.conv_4_model_4_project_bn_running_mean, NumDims.conv_4_model_4_project_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_4_model_4_project_bn_bias, Vars.conv_4_model_4_project_bn_weight, Vars.conv_4_model_4_project_bn_running_mean, Vars.conv_4_model_4_project_bn_running_var, NumDims.x458, NumDims.conv_4_model_4_project_bn_running_mean, NumDims.conv_4_model_4_project_bn_running_var);
if Training
    [Vars.x459, dsmean, dsvar] = batchnorm(Vars.x458, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_4_model_4_project_bn_running_mean = dlarray(dsmean);
    Vars.conv_4_model_4_project_bn_running_var = dlarray(dsvar);
else
    Vars.x459 = batchnorm(Vars.x458, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_4_model_4_project_bn_running_mean = Vars.conv_4_model_4_project_bn_running_mean;
state.conv_4_model_4_project_bn_running_var = Vars.conv_4_model_4_project_bn_running_var;

% Add:
Vars.x460 = Vars.x449 + Vars.x459;
NumDims.x460 = max(NumDims.x449, NumDims.x459);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x461] = prepareConvArgs(Vars.conv_4_model_5_conv_conv_weight, '', Vars.ConvStride4602, Vars.ConvDilationFactor4603, Vars.ConvPadding4604, 1, NumDims.x460, NumDims.conv_4_model_5_conv_conv_weight);
Vars.x461 = dlconv(Vars.x460, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x462, NumDims.conv_4_model_5_conv_bn_running_mean, NumDims.conv_4_model_5_conv_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_4_model_5_conv_bn_bias, Vars.conv_4_model_5_conv_bn_weight, Vars.conv_4_model_5_conv_bn_running_mean, Vars.conv_4_model_5_conv_bn_running_var, NumDims.x461, NumDims.conv_4_model_5_conv_bn_running_mean, NumDims.conv_4_model_5_conv_bn_running_var);
if Training
    [Vars.x462, dsmean, dsvar] = batchnorm(Vars.x461, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_4_model_5_conv_bn_running_mean = dlarray(dsmean);
    Vars.conv_4_model_5_conv_bn_running_var = dlarray(dsvar);
else
    Vars.x462 = batchnorm(Vars.x461, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_4_model_5_conv_bn_running_mean = Vars.conv_4_model_5_conv_bn_running_mean;
state.conv_4_model_5_conv_bn_running_var = Vars.conv_4_model_5_conv_bn_running_var;

% PRelu:
Vars.x464 = max(0,Vars.x462) + Vars.x547.*min(0,Vars.x462);
NumDims.x464 = NumDims.x462;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x465] = prepareConvArgs(Vars.conv_4_model_5_conv_dw_conv_weight, '', Vars.ConvStride4605, Vars.ConvDilationFactor4606, Vars.ConvPadding4607, 256, NumDims.x464, NumDims.conv_4_model_5_conv_dw_conv_weight);
Vars.x465 = dlconv(Vars.x464, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x466, NumDims.conv_4_model_5_conv_dw_bn_running_mean, NumDims.conv_4_model_5_conv_dw_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_4_model_5_conv_dw_bn_bias, Vars.conv_4_model_5_conv_dw_bn_weight, Vars.conv_4_model_5_conv_dw_bn_running_mean, Vars.conv_4_model_5_conv_dw_bn_running_var, NumDims.x465, NumDims.conv_4_model_5_conv_dw_bn_running_mean, NumDims.conv_4_model_5_conv_dw_bn_running_var);
if Training
    [Vars.x466, dsmean, dsvar] = batchnorm(Vars.x465, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_4_model_5_conv_dw_bn_running_mean = dlarray(dsmean);
    Vars.conv_4_model_5_conv_dw_bn_running_var = dlarray(dsvar);
else
    Vars.x466 = batchnorm(Vars.x465, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_4_model_5_conv_dw_bn_running_mean = Vars.conv_4_model_5_conv_dw_bn_running_mean;
state.conv_4_model_5_conv_dw_bn_running_var = Vars.conv_4_model_5_conv_dw_bn_running_var;

% PRelu:
Vars.x468 = max(0,Vars.x466) + Vars.x548.*min(0,Vars.x466);
NumDims.x468 = NumDims.x466;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x469] = prepareConvArgs(Vars.conv_4_model_5_project_conv_weight, '', Vars.ConvStride4608, Vars.ConvDilationFactor4609, Vars.ConvPadding4610, 1, NumDims.x468, NumDims.conv_4_model_5_project_conv_weight);
Vars.x469 = dlconv(Vars.x468, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x470, NumDims.conv_4_model_5_project_bn_running_mean, NumDims.conv_4_model_5_project_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_4_model_5_project_bn_bias, Vars.conv_4_model_5_project_bn_weight, Vars.conv_4_model_5_project_bn_running_mean, Vars.conv_4_model_5_project_bn_running_var, NumDims.x469, NumDims.conv_4_model_5_project_bn_running_mean, NumDims.conv_4_model_5_project_bn_running_var);
if Training
    [Vars.x470, dsmean, dsvar] = batchnorm(Vars.x469, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_4_model_5_project_bn_running_mean = dlarray(dsmean);
    Vars.conv_4_model_5_project_bn_running_var = dlarray(dsvar);
else
    Vars.x470 = batchnorm(Vars.x469, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_4_model_5_project_bn_running_mean = Vars.conv_4_model_5_project_bn_running_mean;
state.conv_4_model_5_project_bn_running_var = Vars.conv_4_model_5_project_bn_running_var;

% Add:
Vars.x471 = Vars.x460 + Vars.x470;
NumDims.x471 = max(NumDims.x460, NumDims.x470);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x472] = prepareConvArgs(Vars.conv_45_conv_conv_weight, '', Vars.ConvStride4611, Vars.ConvDilationFactor4612, Vars.ConvPadding4613, 1, NumDims.x471, NumDims.conv_45_conv_conv_weight);
Vars.x472 = dlconv(Vars.x471, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x473, NumDims.conv_45_conv_bn_running_mean, NumDims.conv_45_conv_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_45_conv_bn_bias, Vars.conv_45_conv_bn_weight, Vars.conv_45_conv_bn_running_mean, Vars.conv_45_conv_bn_running_var, NumDims.x472, NumDims.conv_45_conv_bn_running_mean, NumDims.conv_45_conv_bn_running_var);
if Training
    [Vars.x473, dsmean, dsvar] = batchnorm(Vars.x472, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_45_conv_bn_running_mean = dlarray(dsmean);
    Vars.conv_45_conv_bn_running_var = dlarray(dsvar);
else
    Vars.x473 = batchnorm(Vars.x472, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_45_conv_bn_running_mean = Vars.conv_45_conv_bn_running_mean;
state.conv_45_conv_bn_running_var = Vars.conv_45_conv_bn_running_var;

% PRelu:
Vars.x475 = max(0,Vars.x473) + Vars.x549.*min(0,Vars.x473);
NumDims.x475 = NumDims.x473;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x476] = prepareConvArgs(Vars.conv_45_conv_dw_conv_weight, '', Vars.ConvStride4614, Vars.ConvDilationFactor4615, Vars.ConvPadding4616, 512, NumDims.x475, NumDims.conv_45_conv_dw_conv_weight);
Vars.x476 = dlconv(Vars.x475, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x477, NumDims.conv_45_conv_dw_bn_running_mean, NumDims.conv_45_conv_dw_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_45_conv_dw_bn_bias, Vars.conv_45_conv_dw_bn_weight, Vars.conv_45_conv_dw_bn_running_mean, Vars.conv_45_conv_dw_bn_running_var, NumDims.x476, NumDims.conv_45_conv_dw_bn_running_mean, NumDims.conv_45_conv_dw_bn_running_var);
if Training
    [Vars.x477, dsmean, dsvar] = batchnorm(Vars.x476, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_45_conv_dw_bn_running_mean = dlarray(dsmean);
    Vars.conv_45_conv_dw_bn_running_var = dlarray(dsvar);
else
    Vars.x477 = batchnorm(Vars.x476, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_45_conv_dw_bn_running_mean = Vars.conv_45_conv_dw_bn_running_mean;
state.conv_45_conv_dw_bn_running_var = Vars.conv_45_conv_dw_bn_running_var;

% PRelu:
Vars.x479 = max(0,Vars.x477) + Vars.x550.*min(0,Vars.x477);
NumDims.x479 = NumDims.x477;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x480] = prepareConvArgs(Vars.conv_45_project_conv_weight, '', Vars.ConvStride4617, Vars.ConvDilationFactor4618, Vars.ConvPadding4619, 1, NumDims.x479, NumDims.conv_45_project_conv_weight);
Vars.x480 = dlconv(Vars.x479, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x481, NumDims.conv_45_project_bn_running_mean, NumDims.conv_45_project_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_45_project_bn_bias, Vars.conv_45_project_bn_weight, Vars.conv_45_project_bn_running_mean, Vars.conv_45_project_bn_running_var, NumDims.x480, NumDims.conv_45_project_bn_running_mean, NumDims.conv_45_project_bn_running_var);
if Training
    [Vars.x481, dsmean, dsvar] = batchnorm(Vars.x480, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_45_project_bn_running_mean = dlarray(dsmean);
    Vars.conv_45_project_bn_running_var = dlarray(dsvar);
else
    Vars.x481 = batchnorm(Vars.x480, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_45_project_bn_running_mean = Vars.conv_45_project_bn_running_mean;
state.conv_45_project_bn_running_var = Vars.conv_45_project_bn_running_var;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x482] = prepareConvArgs(Vars.conv_5_model_0_conv_conv_weight, '', Vars.ConvStride4620, Vars.ConvDilationFactor4621, Vars.ConvPadding4622, 1, NumDims.x481, NumDims.conv_5_model_0_conv_conv_weight);
Vars.x482 = dlconv(Vars.x481, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x483, NumDims.conv_5_model_0_conv_bn_running_mean, NumDims.conv_5_model_0_conv_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_5_model_0_conv_bn_bias, Vars.conv_5_model_0_conv_bn_weight, Vars.conv_5_model_0_conv_bn_running_mean, Vars.conv_5_model_0_conv_bn_running_var, NumDims.x482, NumDims.conv_5_model_0_conv_bn_running_mean, NumDims.conv_5_model_0_conv_bn_running_var);
if Training
    [Vars.x483, dsmean, dsvar] = batchnorm(Vars.x482, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_5_model_0_conv_bn_running_mean = dlarray(dsmean);
    Vars.conv_5_model_0_conv_bn_running_var = dlarray(dsvar);
else
    Vars.x483 = batchnorm(Vars.x482, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_5_model_0_conv_bn_running_mean = Vars.conv_5_model_0_conv_bn_running_mean;
state.conv_5_model_0_conv_bn_running_var = Vars.conv_5_model_0_conv_bn_running_var;

% PRelu:
Vars.x485 = max(0,Vars.x483) + Vars.x551.*min(0,Vars.x483);
NumDims.x485 = NumDims.x483;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x486] = prepareConvArgs(Vars.conv_5_model_0_conv_dw_conv_weight, '', Vars.ConvStride4623, Vars.ConvDilationFactor4624, Vars.ConvPadding4625, 256, NumDims.x485, NumDims.conv_5_model_0_conv_dw_conv_weight);
Vars.x486 = dlconv(Vars.x485, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x487, NumDims.conv_5_model_0_conv_dw_bn_running_mean, NumDims.conv_5_model_0_conv_dw_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_5_model_0_conv_dw_bn_bias, Vars.conv_5_model_0_conv_dw_bn_weight, Vars.conv_5_model_0_conv_dw_bn_running_mean, Vars.conv_5_model_0_conv_dw_bn_running_var, NumDims.x486, NumDims.conv_5_model_0_conv_dw_bn_running_mean, NumDims.conv_5_model_0_conv_dw_bn_running_var);
if Training
    [Vars.x487, dsmean, dsvar] = batchnorm(Vars.x486, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_5_model_0_conv_dw_bn_running_mean = dlarray(dsmean);
    Vars.conv_5_model_0_conv_dw_bn_running_var = dlarray(dsvar);
else
    Vars.x487 = batchnorm(Vars.x486, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_5_model_0_conv_dw_bn_running_mean = Vars.conv_5_model_0_conv_dw_bn_running_mean;
state.conv_5_model_0_conv_dw_bn_running_var = Vars.conv_5_model_0_conv_dw_bn_running_var;

% PRelu:
Vars.x489 = max(0,Vars.x487) + Vars.x552.*min(0,Vars.x487);
NumDims.x489 = NumDims.x487;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x490] = prepareConvArgs(Vars.conv_5_model_0_project_conv_weight, '', Vars.ConvStride4626, Vars.ConvDilationFactor4627, Vars.ConvPadding4628, 1, NumDims.x489, NumDims.conv_5_model_0_project_conv_weight);
Vars.x490 = dlconv(Vars.x489, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x491, NumDims.conv_5_model_0_project_bn_running_mean, NumDims.conv_5_model_0_project_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_5_model_0_project_bn_bias, Vars.conv_5_model_0_project_bn_weight, Vars.conv_5_model_0_project_bn_running_mean, Vars.conv_5_model_0_project_bn_running_var, NumDims.x490, NumDims.conv_5_model_0_project_bn_running_mean, NumDims.conv_5_model_0_project_bn_running_var);
if Training
    [Vars.x491, dsmean, dsvar] = batchnorm(Vars.x490, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_5_model_0_project_bn_running_mean = dlarray(dsmean);
    Vars.conv_5_model_0_project_bn_running_var = dlarray(dsvar);
else
    Vars.x491 = batchnorm(Vars.x490, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_5_model_0_project_bn_running_mean = Vars.conv_5_model_0_project_bn_running_mean;
state.conv_5_model_0_project_bn_running_var = Vars.conv_5_model_0_project_bn_running_var;

% Add:
Vars.x492 = Vars.x481 + Vars.x491;
NumDims.x492 = max(NumDims.x481, NumDims.x491);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x493] = prepareConvArgs(Vars.conv_5_model_1_conv_conv_weight, '', Vars.ConvStride4629, Vars.ConvDilationFactor4630, Vars.ConvPadding4631, 1, NumDims.x492, NumDims.conv_5_model_1_conv_conv_weight);
Vars.x493 = dlconv(Vars.x492, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x494, NumDims.conv_5_model_1_conv_bn_running_mean, NumDims.conv_5_model_1_conv_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_5_model_1_conv_bn_bias, Vars.conv_5_model_1_conv_bn_weight, Vars.conv_5_model_1_conv_bn_running_mean, Vars.conv_5_model_1_conv_bn_running_var, NumDims.x493, NumDims.conv_5_model_1_conv_bn_running_mean, NumDims.conv_5_model_1_conv_bn_running_var);
if Training
    [Vars.x494, dsmean, dsvar] = batchnorm(Vars.x493, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_5_model_1_conv_bn_running_mean = dlarray(dsmean);
    Vars.conv_5_model_1_conv_bn_running_var = dlarray(dsvar);
else
    Vars.x494 = batchnorm(Vars.x493, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_5_model_1_conv_bn_running_mean = Vars.conv_5_model_1_conv_bn_running_mean;
state.conv_5_model_1_conv_bn_running_var = Vars.conv_5_model_1_conv_bn_running_var;

% PRelu:
Vars.x496 = max(0,Vars.x494) + Vars.x553.*min(0,Vars.x494);
NumDims.x496 = NumDims.x494;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x497] = prepareConvArgs(Vars.conv_5_model_1_conv_dw_conv_weight, '', Vars.ConvStride4632, Vars.ConvDilationFactor4633, Vars.ConvPadding4634, 256, NumDims.x496, NumDims.conv_5_model_1_conv_dw_conv_weight);
Vars.x497 = dlconv(Vars.x496, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x498, NumDims.conv_5_model_1_conv_dw_bn_running_mean, NumDims.conv_5_model_1_conv_dw_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_5_model_1_conv_dw_bn_bias, Vars.conv_5_model_1_conv_dw_bn_weight, Vars.conv_5_model_1_conv_dw_bn_running_mean, Vars.conv_5_model_1_conv_dw_bn_running_var, NumDims.x497, NumDims.conv_5_model_1_conv_dw_bn_running_mean, NumDims.conv_5_model_1_conv_dw_bn_running_var);
if Training
    [Vars.x498, dsmean, dsvar] = batchnorm(Vars.x497, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_5_model_1_conv_dw_bn_running_mean = dlarray(dsmean);
    Vars.conv_5_model_1_conv_dw_bn_running_var = dlarray(dsvar);
else
    Vars.x498 = batchnorm(Vars.x497, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_5_model_1_conv_dw_bn_running_mean = Vars.conv_5_model_1_conv_dw_bn_running_mean;
state.conv_5_model_1_conv_dw_bn_running_var = Vars.conv_5_model_1_conv_dw_bn_running_var;

% PRelu:
Vars.x500 = max(0,Vars.x498) + Vars.x554.*min(0,Vars.x498);
NumDims.x500 = NumDims.x498;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x501] = prepareConvArgs(Vars.conv_5_model_1_project_conv_weight, '', Vars.ConvStride4635, Vars.ConvDilationFactor4636, Vars.ConvPadding4637, 1, NumDims.x500, NumDims.conv_5_model_1_project_conv_weight);
Vars.x501 = dlconv(Vars.x500, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x502, NumDims.conv_5_model_1_project_bn_running_mean, NumDims.conv_5_model_1_project_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_5_model_1_project_bn_bias, Vars.conv_5_model_1_project_bn_weight, Vars.conv_5_model_1_project_bn_running_mean, Vars.conv_5_model_1_project_bn_running_var, NumDims.x501, NumDims.conv_5_model_1_project_bn_running_mean, NumDims.conv_5_model_1_project_bn_running_var);
if Training
    [Vars.x502, dsmean, dsvar] = batchnorm(Vars.x501, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_5_model_1_project_bn_running_mean = dlarray(dsmean);
    Vars.conv_5_model_1_project_bn_running_var = dlarray(dsvar);
else
    Vars.x502 = batchnorm(Vars.x501, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_5_model_1_project_bn_running_mean = Vars.conv_5_model_1_project_bn_running_mean;
state.conv_5_model_1_project_bn_running_var = Vars.conv_5_model_1_project_bn_running_var;

% Add:
Vars.x503 = Vars.x492 + Vars.x502;
NumDims.x503 = max(NumDims.x492, NumDims.x502);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x504] = prepareConvArgs(Vars.conv_6_sep_conv_weight, '', Vars.ConvStride4638, Vars.ConvDilationFactor4639, Vars.ConvPadding4640, 1, NumDims.x503, NumDims.conv_6_sep_conv_weight);
Vars.x504 = dlconv(Vars.x503, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x505, NumDims.conv_6_sep_bn_running_mean, NumDims.conv_6_sep_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_6_sep_bn_bias, Vars.conv_6_sep_bn_weight, Vars.conv_6_sep_bn_running_mean, Vars.conv_6_sep_bn_running_var, NumDims.x504, NumDims.conv_6_sep_bn_running_mean, NumDims.conv_6_sep_bn_running_var);
if Training
    [Vars.x505, dsmean, dsvar] = batchnorm(Vars.x504, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_6_sep_bn_running_mean = dlarray(dsmean);
    Vars.conv_6_sep_bn_running_var = dlarray(dsvar);
else
    Vars.x505 = batchnorm(Vars.x504, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_6_sep_bn_running_mean = Vars.conv_6_sep_bn_running_mean;
state.conv_6_sep_bn_running_var = Vars.conv_6_sep_bn_running_var;

% PRelu:
Vars.x507 = max(0,Vars.x505) + Vars.x555.*min(0,Vars.x505);
NumDims.x507 = NumDims.x505;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x508] = prepareConvArgs(Vars.conv_6_dw_conv_weight, '', Vars.ConvStride4641, Vars.ConvDilationFactor4642, Vars.ConvPadding4643, 512, NumDims.x507, NumDims.conv_6_dw_conv_weight);
Vars.x508 = dlconv(Vars.x507, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x509, NumDims.conv_6_dw_bn_running_mean, NumDims.conv_6_dw_bn_running_var] = prepareBatchNormalizationArgs(Vars.conv_6_dw_bn_bias, Vars.conv_6_dw_bn_weight, Vars.conv_6_dw_bn_running_mean, Vars.conv_6_dw_bn_running_var, NumDims.x508, NumDims.conv_6_dw_bn_running_mean, NumDims.conv_6_dw_bn_running_var);
if Training
    [Vars.x509, dsmean, dsvar] = batchnorm(Vars.x508, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.conv_6_dw_bn_running_mean = dlarray(dsmean);
    Vars.conv_6_dw_bn_running_var = dlarray(dsvar);
else
    Vars.x509 = batchnorm(Vars.x508, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.conv_6_dw_bn_running_mean = Vars.conv_6_dw_bn_running_mean;
state.conv_6_dw_bn_running_var = Vars.conv_6_dw_bn_running_var;

% Shape:
[Vars.x510, NumDims.x510] = onnxShape(Vars.x509, NumDims.x509);

% Gather:
[Vars.x512, NumDims.x512] = onnxGather(Vars.x510, Vars.x511, 0, NumDims.x510, NumDims.x511);

% Unsqueeze:
[shape, NumDims.x514] = prepareUnsqueezeArgs(Vars.x512, Vars.UnsqueezeAxes4644, NumDims.x512);
Vars.x514 = reshape(Vars.x512, shape);

% Concat:
[dim, NumDims.x516] = prepareConcatArgs(0, [NumDims.x514, NumDims.x556]);
Vars.x516 = cat(dim, Vars.x514, Vars.x556);

% Reshape:
[shape, NumDims.x517] = prepareReshapeArgs(Vars.x509, Vars.x516, NumDims.x509);
Vars.x517 = reshape(Vars.x509, shape{:});

% MatMul:
[Vars.x519, NumDims.x519] = onnxMatMul(Vars.x517, Vars.x557, NumDims.x517, NumDims.x557);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x520, NumDims.bn_running_mean, NumDims.bn_running_var] = prepareBatchNormalizationArgs(Vars.bn_bias, Vars.bn_weight, Vars.bn_running_mean, Vars.bn_running_var, NumDims.x519, NumDims.bn_running_mean, NumDims.bn_running_var);
if Training
    [Vars.x520, dsmean, dsvar] = batchnorm(Vars.x519, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.bn_running_mean = dlarray(dsmean);
    Vars.bn_running_var = dlarray(dsvar);
else
    Vars.x520 = batchnorm(Vars.x519, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.bn_running_mean = Vars.bn_running_mean;
state.bn_running_var = Vars.bn_running_var;

% ReduceL2:
dims = prepareReduceArgs(Vars.ReduceL2Axes4645, NumDims.x520);
Vars.x521 = sqrt(sum(Vars.x520.^2, dims));
NumDims.x521 = NumDims.x520;

% Div:
Vars.x522 = Vars.x520 ./ Vars.x521;
NumDims.x522 = max(NumDims.x520, NumDims.x521);

% Set graph output arguments from Vars and NumDims:
x522 = Vars.x522;
x522NumDims4647 = NumDims.x522;
% Set output state from Vars:
state = updateStruct(state, Vars);
end

function [inputDataPerms, outputDataPerms, Training] = parseInputs(input_1, numDataOutputs, params, varargin)
% Function to validate inputs to faceRecFcn:
p = inputParser;
isValidArrayInput = @(x)isnumeric(x) || isstring(x);
isValidONNXParameters = @(x)isa(x, 'ONNXParameters');
addRequired(p, 'input_1', isValidArrayInput);
addRequired(p, 'params', isValidONNXParameters);
addParameter(p, 'InputDataPermutation', 'auto');
addParameter(p, 'OutputDataPermutation', 'auto');
addParameter(p, 'Training', false);
parse(p, input_1, params, varargin{:});
inputDataPerms = p.Results.InputDataPermutation;
outputDataPerms = p.Results.OutputDataPermutation;
Training = p.Results.Training;
if isnumeric(inputDataPerms)
    inputDataPerms = {inputDataPerms};
end
if isstring(inputDataPerms) && isscalar(inputDataPerms) || ischar(inputDataPerms)
    inputDataPerms = repmat({inputDataPerms},1,1);
end
if isnumeric(outputDataPerms)
    outputDataPerms = {outputDataPerms};
end
if isstring(outputDataPerms) && isscalar(outputDataPerms) || ischar(outputDataPerms)
    outputDataPerms = repmat({outputDataPerms},1,numDataOutputs);
end
end

function [input_1, Training, outputDataPerms, anyDlarrayInputs] = preprocessInput(input_1, params, varargin)
% Parse input arguments
[inputDataPerms, outputDataPerms, Training] = parseInputs(input_1, 1, params, varargin{:});
anyDlarrayInputs = any(cellfun(@(x)isa(x, 'dlarray'), {input_1}));
% Make the input variables into unlabelled dlarrays:
input_1 = makeUnlabeledDlarray(input_1);
% Permute inputs if requested:
input_1 = permuteInputVar(input_1, inputDataPerms{1}, 4);
% Check input size(s):
checkInputSize(size(input_1), {1 3 112 112}, "input_1");
end

function [x522] = postprocessOutput(x522, outputDataPerms, anyDlarrayInputs, Training, varargin)
% Set output type:
if ~anyDlarrayInputs && ~Training
    x522 = extractdata(x522);
end
% Permute outputs if requested:
x522 = permuteOutputVar(x522, outputDataPerms{1}, 2);
end


%% dlarray functions implementing ONNX operators:

function [Y, numDimsY] = onnxGather(X, ONNXIdx, ONNXAxis, numDimsX, numDimsIdx)
% Function implementing the ONNX Gather operator

% In ONNX, 'Gather' first indexes into dimension ONNXAxis of data, using
% the contents of ONNXIdx as the indices. Then, it reshapes the ONNXAxis
% into the shape of ONNXIdx.
%   Example 1:
% Suppose data has shape [2 3 4 5], ONNXIdx has shape [6 7], and axis=1.
% The result has shape [2 6 7 4 5].
%   Example 2:
% Suppose data has shape [2 3 4 5], ONNXIdx has shape [6], and axis=1.
% The result has shape [2 6 4 5].
%   Example 3:
% Suppose data has shape [2 3 4 5], ONNXIdx has shape [] (a scalar), and axis=1.
% The result has shape [2 4 5].
%
% Since we're using reverse indexing relative to ONNX, in this function
% data and ONNXIdx both have reversed dimension ordering.
numDimsY = numDimsIdx + (numDimsX - 1);
if isempty(X)
    Y = X;
    return;
end
% (1) First, do the subsref part of Gather
if ONNXAxis<0
    ONNXAxis = ONNXAxis + numDimsX;                                 % Axis can be negative. Convert it to its positive equivalent.
end
% ONNXIdx is origin-0 in ONNX, so add 1
mlIdx  = extractdata(ONNXIdx) + 1;
% Convert axis to DLT. ONNXAxis is origin 0 and we index from the end
mlAxis = numDimsX - ONNXAxis;
% Use subsref to index into data
Indices.subs = repmat({':'}, 1, numDimsX);
Indices.subs{mlAxis} = mlIdx(:);                                          % Index as a column to ensure the output is 1-D in the indexed dimension (for now).
Indices.type = '()';
Y = subsref(X, Indices);
% (2) Now do the reshaping part of Gather
shape = size(Y, 1:numDimsX);
if numDimsIdx == 0
    % Delete the indexed dimension
    shape(mlAxis) = [];
elseif numDimsIdx > 1
    % Reshape the indexed dimension into the shape of ONNXIdx
    shape = [shape(1:mlAxis-1) size(ONNXIdx, 1:numDimsIdx) shape(mlAxis+1:end)];
end
% Extend the shape to 2D so it's valid MATLAB
if numel(shape) < 2
    shape = [shape ones(1,2-numel(shape))];
end
Y = reshape(Y, shape);
end

function [D, numDimsD] = onnxMatMul(A, B, numDimsA, numDimsB)
% Implements the ONNX MatMul operator.

% If either arg is more than 2D, loop over all dimensions before the final
% 2. Inside the loop, perform matrix multiplication.

% If B is 1-D, temporarily extend it to a row vector
if numDimsB==1
    B = B(:)';
end
maxNumDims = max(numDimsA, numDimsB);
numDimsD = maxNumDims;
if maxNumDims > 2
    % sizes of matrices to be multiplied
    matSizeA        = size(A, 1:2);
    matSizeB        = size(B, 1:2);
    % size of the stack of matrices
    stackSizeA      = size(A, 3:maxNumDims);
    stackSizeB      = size(B, 3:maxNumDims);
    % final stack size
    resultStackSize = max(stackSizeA, stackSizeB);
    % full implicitly-expanded sizes
    fullSizeA       = [matSizeA resultStackSize];
    fullSizeB       = [matSizeB resultStackSize];
    resultSize      = [matSizeB(1) matSizeA(2) resultStackSize];
    % Repmat A and B up to the full stack size using implicit expansion
    A = A + zeros(fullSizeA);
    B = B + zeros(fullSizeB);
    % Reshape A and B to flatten the stack dims (all dims after the first 2)
    A2 = reshape(A, size(A,1), size(A,2), []);
    B2 = reshape(B, size(B,1), size(B,2), []);
    % Iterate down the stack dim, doing the 2d matrix multiplications
    D2 = zeros([matSizeB(1), matSizeA(2), size(A2,3)], 'like', A);
    for i = size(A2,3):-1:1
        D2(:,:,i) = B2(:,:,i) * A2(:,:,i);
    end
    % Reshape D2 to the result size (unflatten the stack dims)
    D = reshape(D2, resultSize);
else
    D = B * A;
    if numDimsA==1 || numDimsB==1
        D = D(:);
        numDimsD = 1;
    end
end
end

function [Y, numDimsY] = onnxShape(X, numDimsX)
% Implements the ONNX Shape operator
% Return the reverse ONNX shape as a 1D column vector
switch numDimsX
    case 0
        if isempty(X)
            Y = dlarray(0);
        else
            Y = dlarray(1);
        end
    case 1
        if isempty(X)
            Y = dlarray(0);
        else
            Y = dlarray(size(X,1));
        end
    otherwise
        Y = dlarray(fliplr(size(X, 1:numDimsX))');
end
numDimsY = 1;
end

function [offset, scale, datasetMean, datasetVariance, dataFormat, numDimsY, numDimsDatasetMean, numDimsDatasetVariance] = prepareBatchNormalizationArgs(...
    offset, scale, datasetMean, datasetVariance, numDimsX, numDimsDatasetMean, numDimsDatasetVariance)
% Prepares arguments for implementing the ONNX BatchNormalization operator
offset = dlarray(offset,'C');
scale = dlarray(scale,'C');
datasetMean = extractdata(datasetMean);
datasetVariance = extractdata(datasetVariance);
datasetVariance(datasetVariance <= 0) = realmin('single');  % Set nonpositive variance components to a value below eps('single')
dataFormat = [repmat('U', 1, numDimsX-2), 'CB'];
numDimsY = numDimsX;
end

function [DLTAxis, numDimsY] = prepareConcatArgs(ONNXAxis, numDimsXs)
% Prepares arguments for implementing the ONNX Concat operator
numDimsY = numDimsXs(1);
if ONNXAxis<0
    ONNXAxis = ONNXAxis + numDimsY;
end
DLTAxis = numDimsY - ONNXAxis;
end

function [weights, bias, stride, dilationFactor, padding, dataFormat, numDimsY] = prepareConvArgs(...
    weights, bias, stride, dilationFactor, padding, numWtGroups, numDimsX, numDimsW)
% Prepares arguments for implementing the ONNX Conv operator

% Weights: The ONNX weight dim is Fcxyz..., where c=C/G, G is numGroups,
% and xyz... are spatial dimensions. DLT "weights" here is the flip of
% that, or ...zyxcF. dlconv requires ...zyxcfG, where f=F/G. So reshape to
% split the last dimension.
sizeW    = size(weights, 1:numDimsW);
F        = sizeW(end);
newWSize = [sizeW(1:numDimsW-1), F/numWtGroups, numWtGroups];
weights  = reshape(weights, newWSize);
% bias
if isempty(bias)
    bias = 0;
end
bias = dlarray(bias(:),'CU');
% Make the attributes non-dlarrays:
if isa(stride, 'dlarray')
    stride = extractdata(stride);
end
if isa(dilationFactor, 'dlarray')
    dilationFactor = extractdata(dilationFactor);
end
if isa(padding, 'dlarray')
    padding = extractdata(padding);
end
% Make the attributes doubles:
stride = double(stride);
dilationFactor = double(dilationFactor);
if isnumeric(padding)       % padding can be "same"
    padding = double(padding);
end
% Set dataformat and numdims
dataFormat = [repmat('S', 1, numDimsX-2) 'CB'];
numDimsY = numDimsX;
end

function dims = prepareReduceArgs(ONNXAxes, numDimsX)
% Prepares arguments for implementing the ONNX Reduce operator
if isempty(ONNXAxes)
    ONNXAxes = 0:numDimsX-1;   % All axes
end
ONNXAxes(ONNXAxes<0) = ONNXAxes(ONNXAxes<0) + numDimsX;
dims = numDimsX - ONNXAxes;
end

function [DLTShape, numDimsY] = prepareReshapeArgs(X, ONNXShape, numDimsX)
% Prepares arguments for implementing the ONNX Reshape operator
ONNXShape = flip(extractdata(ONNXShape));            % First flip the shape to make it correspond to the dimensions of X.
% In ONNX, 0 means "unchanged", and -1 means "infer". In DLT, there is no
% "unchanged", and [] means "infer".
DLTShape = num2cell(ONNXShape);                      % Make a cell array so we can include [].
% Replace zeros with the actual size
if any(ONNXShape==0)
    i0 = find(ONNXShape==0);
    DLTShape(i0) = num2cell(size(X, numDimsX - numel(ONNXShape) + i0));  % right-align the shape vector and dims
end
if any(ONNXShape == -1)
    % Replace -1 with []
    i = ONNXShape == -1;
    DLTShape{i} = [];
end
if numel(DLTShape)==1
    DLTShape = [DLTShape 1];
end
numDimsY = numel(ONNXShape);
end

function [newShape, numDimsY] = prepareUnsqueezeArgs(X, ONNXAxes, numDimsX)
% Prepares arguments for implementing the ONNX Unsqueeze operator

% ONNX axes are origin 0
ONNXAxes = extractdata(ONNXAxes);
ONNXAxes(ONNXAxes<0) = ONNXAxes(ONNXAxes<0) + numDimsX;
ONNXAxes = sort(ONNXAxes);                                              % increasing order
numDimsY = numDimsX + numel(ONNXAxes);
if numDimsY == 1
    newShape = size(X);
else
    DLTAxes  = flip(numDimsY - ONNXAxes);                                  % increasing order
    newShape = ones(1, numDimsY);
    posToSet = setdiff(1:numDimsY, DLTAxes, 'stable');
    newShape(posToSet) = size(X, 1:numel(posToSet));
end
end

%% Utility functions:

function s = appendStructs(varargin)
% s = appendStructs(s1, s2,...). Assign all fields in s1, s2,... into s.
if isempty(varargin)
    s = struct;
else
    s = varargin{1};
    for i = 2:numel(varargin)
        fromstr = varargin{i};
        fs = fieldnames(fromstr);
        for j = 1:numel(fs)
            s.(fs{j}) = fromstr.(fs{j});
        end
    end
end
end

function checkInputSize(inputShape, expectedShape, inputName)

% The input dimensions have been reversed; flip them back to compare to the
% expected ONNX shape.
inputShape = fliplr(inputShape);

% Check whether the expected shape is 0 or 1D. If so, expand the expected size.
if isempty(expectedShape)
    expectedShape = {1, 1};
elseif numel(expectedShape)==1
    expectedShape{2} = 1;
end

% If the expected shape has fewer dims than the input shape, error.
if numel(expectedShape) < numel(inputShape)
    expectedSizeStr = strjoin(["[", strjoin(string(expectedShape), ","), "]"], "");
    error(message('nnet_cnn_onnx:onnx:InputHasGreaterNDims', inputName, expectedSizeStr));
end

% Prepad the input shape with trailing ones up to the number of elements in
% expectedShape
inputShape = num2cell([ones(1, numel(expectedShape) - length(inputShape)) inputShape]);

% Find the number of variable size dimensions in the expected shape
numVariableInputs = sum(cellfun(@(x) isa(x, 'char') || isa(x, 'string'), expectedShape));

% Find the number of input dimensions that are not in the expected shape
% and cannot be represented by a variable dimension
nonMatchingInputDims = setdiff(string(inputShape), string(expectedShape));
numNonMatchingInputDims  = numel(nonMatchingInputDims) - numVariableInputs;

expectedSizeStr = strjoin(["[", strjoin(string(expectedShape), ","), "]"], "");
inputSizeStr = strjoin(["[", strjoin(string(inputShape), ","), "]"], "");
if numNonMatchingInputDims == 0 && ~iSizesMatch(inputShape, expectedShape)
    % The actual and expected input dimensions match, but in
    % a different order. The input needs to be permuted.
    error(message('nnet_cnn_onnx:onnx:InputNeedsPermute',inputName, expectedSizeStr, inputSizeStr));
elseif numNonMatchingInputDims > 0
    % The actual and expected input sizes do not match.
    error(message('nnet_cnn_onnx:onnx:InputNeedsResize',inputName, expectedSizeStr, inputSizeStr));
end

end

function doesMatch = iSizesMatch(inputShape, expectedShape)
% Check whether the input and expected shapes match, in order.
% Size elements match if (1) the elements are equal, or (2) the expected
% size element is a variable (represented by a character vector or string)
doesMatch = true;
for i=1:numel(inputShape)
    if ~(isequal(inputShape{i},expectedShape{i}) || ischar(expectedShape{i}) || isstring(expectedShape{i}))
        doesMatch = false;
        return
    end
end
end

function X = makeUnlabeledDlarray(X)
% Make numeric X into an unlabelled dlarray
if isa(X, 'dlarray')
    X = stripdims(X);
elseif isnumeric(X)
    if ~(isa(X,'single') || isa(X,'double'))
        % Make ints double so they can combine with anything without
        % reducting precision
        X = double(X);
    end
    X = dlarray(X);
end
end

function [Vars, NumDims] = packageVariables(params, inputNames, inputValues, inputNumDims)
% inputNames, inputValues are cell arrays. inputRanks is a numeric vector.
Vars = appendStructs(params.Learnables, params.Nonlearnables, params.State);
NumDims = params.NumDimensions;
% Add graph inputs
for i = 1:numel(inputNames)
    Vars.(inputNames{i}) = inputValues{i};
    NumDims.(inputNames{i}) = inputNumDims(i);
end
end

function X = permuteInputVar(X, userDataPerm, onnxNDims)
% Returns reverse-ONNX ordering
if isnumeric(userDataPerm)
    % Permute into reverse ONNX ordering
    perm = fliplr(userDataPerm);
elseif isequal(userDataPerm, 'auto') && onnxNDims == 4
    % Permute MATLAB HWCN to reverse onnx (WHCN)
    perm = [2 1 3 4];
elseif onnxNDims == 0
    return;
else
    % userDataPerm is either 'none' or 'auto' with no default, which means
    % it's already in onnx ordering, so just make it reverse onnx
    perm = max(2,onnxNDims):-1:1;
end
X = permute(X, perm);
end

function Y = permuteOutputVar(Y, userDataPerm, onnxNDims)
switch onnxNDims
    case 0
        perm = [];
    case 1
        if isnumeric(userDataPerm)
            % Use the user's permutation because Y is a column vector which
            % already matches ONNX.
            perm = userDataPerm;
        elseif isequal(userDataPerm, 'auto')
            % Treat the 1D onnx vector as a 2D column and transpose it
            perm = [2 1];
        else
            % userDataPerm is 'none'. Leave Y alone because it already
            % matches onnx.
            perm = [];
        end
    otherwise
        % ndims >= 2
        if isnumeric(userDataPerm)
            % Use the inverse of the user's permutation. This is not just the
            % flip of the permutation vector.
            perm = onnxNDims + 1 - userDataPerm;
        elseif isequal(userDataPerm, 'auto')
            if onnxNDims == 2
                % Permute reverse ONNX CN to DLT CN (do nothing)
                perm = [];
            elseif onnxNDims == 4
                % Permute reverse onnx (WHCN) to MATLAB HWCN
                perm = [2 1 3 4];
            else
                % User wants the output in ONNX ordering, so just reverse it from
                % reverse onnx
                perm = onnxNDims:-1:1;
            end
        else
            % userDataPerm is 'none', so just make it reverse onnx
            perm = onnxNDims:-1:1;
        end
end
if ~isempty(perm)
    Y = permute(Y, perm);
end
end

function s = updateStruct(s, t)
% Set all existing fields in s from fields in t, ignoring extra fields in t.
for name = transpose(fieldnames(s))
    s.(name{1}) = t.(name{1});
end
end
